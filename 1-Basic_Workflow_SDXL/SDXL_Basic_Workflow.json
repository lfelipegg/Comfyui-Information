{
  "last_node_id": 45,
  "last_link_id": 73,
  "nodes": [
    {
      "id": 8,
      "type": "VAELoader",
      "pos": [
        836.429931640625,
        109.13655090332031
      ],
      "size": [
        315,
        58
      ],
      "flags": {},
      "order": 0,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "VAE",
          "type": "VAE",
          "links": [
            57
          ],
          "slot_index": 0,
          "shape": 3
        }
      ],
      "properties": {
        "Node name for S&R": "VAELoader"
      },
      "widgets_values": [
        "sdxl_vae.safetensors"
      ]
    },
    {
      "id": 25,
      "type": "KSampler",
      "pos": [
        817.3287963867188,
        634.5413818359375
      ],
      "size": [
        315,
        262
      ],
      "flags": {},
      "order": 13,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 54
        },
        {
          "name": "positive",
          "type": "CONDITIONING",
          "link": 68
        },
        {
          "name": "negative",
          "type": "CONDITIONING",
          "link": 67
        },
        {
          "name": "latent_image",
          "type": "LATENT",
          "link": 65
        }
      ],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "links": [
            70
          ],
          "slot_index": 0,
          "shape": 3
        }
      ],
      "properties": {
        "Node name for S&R": "KSampler"
      },
      "widgets_values": [
        1021703977609964,
        "randomize",
        30,
        8,
        "euler_ancestral",
        "normal",
        1
      ]
    },
    {
      "id": 26,
      "type": "VAEDecode",
      "pos": [
        1316,
        111
      ],
      "size": [
        210,
        46
      ],
      "flags": {},
      "order": 14,
      "mode": 0,
      "inputs": [
        {
          "name": "samples",
          "type": "LATENT",
          "link": 70
        },
        {
          "name": "vae",
          "type": "VAE",
          "link": 57
        }
      ],
      "outputs": [
        {
          "name": "IMAGE",
          "type": "IMAGE",
          "links": [
            71
          ],
          "slot_index": 0,
          "shape": 3
        }
      ],
      "properties": {
        "Node name for S&R": "VAEDecode"
      },
      "widgets_values": []
    },
    {
      "id": 27,
      "type": "SaveImage",
      "pos": [
        1560,
        595
      ],
      "size": [
        704.143798828125,
        604.4697265625
      ],
      "flags": {},
      "order": 15,
      "mode": 0,
      "inputs": [
        {
          "name": "images",
          "type": "IMAGE",
          "link": 71
        }
      ],
      "outputs": [],
      "properties": {
        "Node name for S&R": "SaveImage"
      },
      "widgets_values": [
        "ComfyUI"
      ]
    },
    {
      "id": 35,
      "type": "EmptyLatentImage",
      "pos": [
        429.2620544433594,
        93.86686706542969
      ],
      "size": [
        315,
        106
      ],
      "flags": {},
      "order": 1,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "links": [
            65
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "EmptyLatentImage"
      },
      "widgets_values": [
        1344,
        768,
        1
      ]
    },
    {
      "id": 36,
      "type": "CLIPTextEncode",
      "pos": [
        30.614160537719727,
        760.1495971679688
      ],
      "size": [
        309.0394287109375,
        187.4150390625
      ],
      "flags": {},
      "order": 11,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 72
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "links": [
            68
          ],
          "slot_index": 0
        }
      ],
      "title": "Prompt - Positive",
      "properties": {
        "Node name for S&R": "CLIPTextEncode"
      },
      "widgets_values": [
        "",
        [
          false,
          true
        ]
      ],
      "color": "#232",
      "bgcolor": "#353"
    },
    {
      "id": 37,
      "type": "CLIPTextEncode",
      "pos": [
        373,
        760
      ],
      "size": [
        314.11865234375,
        185.0558624267578
      ],
      "flags": {},
      "order": 12,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 73
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "links": [
            67
          ]
        }
      ],
      "title": "Prompt - Negative",
      "properties": {
        "Node name for S&R": "CLIPTextEncode"
      },
      "widgets_values": [
        "",
        [
          false,
          true
        ]
      ],
      "color": "#322",
      "bgcolor": "#533"
    },
    {
      "id": 38,
      "type": "Note",
      "pos": [
        25,
        628
      ],
      "size": [
        655.8467407226562,
        79.97681427001953
      ],
      "flags": {},
      "order": 2,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "Note - Prompts",
      "properties": {},
      "widgets_values": [
        "Prompts play a crucial role in guiding the AI to generate desired outputs, particularly in the context of image generation models. \n"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 39,
      "type": "Note",
      "pos": [
        30.614160537719727,
        1008.1495361328125
      ],
      "size": [
        310.2938232421875,
        190.2924041748047
      ],
      "flags": {},
      "order": 3,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {},
      "widgets_values": [
        "The positive prompt is a list of words, phrases, or descriptions that you provide to the AI to define the elements or qualities you want to include in the generated image. It acts as a \"guide\" for what the image should contain, emphasizing features such as style, objects, mood, color, composition, or subject matter.\n\nexample: \"a futuristic cityscape, vibrant neon lights, cyberpunk style, rainy night\""
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 40,
      "type": "Note",
      "pos": [
        366,
        1005
      ],
      "size": [
        314.3302917480469,
        182.32110595703125
      ],
      "flags": {},
      "order": 4,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {},
      "widgets_values": [
        "The negative prompt specifies elements or qualities you want the AI to exclude from the image. This helps refine the output by telling the AI what to avoid, leading to cleaner, more accurate results aligned with your vision.\n\nexample: \"blurry, low quality, overexposed, watermark, text\""
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 41,
      "type": "Note",
      "pos": [
        36.719688415527344,
        238.3946075439453
      ],
      "size": [
        325.179443359375,
        266.24713134765625
      ],
      "flags": {},
      "order": 5,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "Note - Checkpoint",
      "properties": {},
      "widgets_values": [
        "A checkpoint is essentially a trained model file that contains the weights and configurations of a neural network, typically a diffusion model like Stable Diffusion. It is the \"brain\" of the image generation system and defines how the model interprets prompts and generates images.\n\n    Purpose: It determines the style, quality, and overall capability of the model.\n    Types:\n        Base Checkpoints: General-purpose models trained on diverse datasets, like Stable Diffusion v1.5 or v2.1.\n        Fine-tuned Checkpoints: Models fine-tuned for specific styles, themes, or purposes (e.g., anime-style art, photorealism, etc.).\n    File Formats: Commonly .ckpt or .safetensors.\n\nYou can download a SDXL checkpoint in https://civitai.com/models/593865/30104?modelVersionId=995367\n\nPath: ComfyUi/models/checkpoints"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 42,
      "type": "Note",
      "pos": [
        829.429931640625,
        213.1365509033203
      ],
      "size": [
        438.9461364746094,
        240.24237060546875
      ],
      "flags": {},
      "order": 6,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "Note - Vae",
      "properties": {},
      "widgets_values": [
        "A VAE is a separate but complementary component often used with diffusion models like Stable Diffusion. It is responsible for encoding and decoding the image data during generation.\n\n    Purpose: VAEs compress (encode) the high-dimensional image information into a lower-dimensional space and reconstruct (decode) it back into high-quality images. This helps to:\n        Improve image clarity and detail.\n        Handle color and texture better.\n    Impact: Using a well-trained VAE can significantly enhance the sharpness, color, and overall quality of generated images.\n    Customization: Some checkpoints come with a built-in VAE, while others require you to load an external VAE file for optimal results.\n\nYou can download the SDXL vae from https://civitai.com/models/296576/sdxl-vae\n\npath: ComfyUi/models/vae"
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 43,
      "type": "Note",
      "pos": [
        431.3089904785156,
        249.541259765625
      ],
      "size": [
        325.10504150390625,
        267.56475830078125
      ],
      "flags": {},
      "order": 7,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "Note - Latent Image",
      "properties": {},
      "widgets_values": [
        "In the context of AI image generation and diffusion models like those used in ComfyUI or Stable Diffusion, the term latent image refers to the compressed representation of an image in the latent space.\n\nWhat is Latent Space?\n- Latent space is a lower-dimensional representation of the data (in this case, images) used by machine learning models to process and manipulate information more efficiently.\n- Instead of working directly with high-resolution pixel data, the model encodes the image into a smaller, abstract mathematical form called a latent representation.\n\nWhat is a Latent Image?\nA latent image is the representation of an image in this compressed latent space. It is not directly viewable as a normal image because it exists as abstract data. However:\n-It contains all the essential features and structure of the image.\n-It enables the model to process and generate images more efficiently."
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 44,
      "type": "Note",
      "pos": [
        1145.0069580078125,
        629.1514892578125
      ],
      "size": [
        335.048828125,
        635.5007934570312
      ],
      "flags": {},
      "order": 8,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "Note - Ksampler",
      "properties": {},
      "widgets_values": [
        "In ComfyUI and other diffusion-based image generation tools, the KSampler is a node or module used to control the denoising process during the generation of images. It plays a key role in refining and guiding how random noise in latent space is transformed into a coherent image.\n\nWhat Does KSampler Do?\n1.- Sampling:\n- The KSampler applies a specific sampling algorithm to \"denoise\" the latent space step-by-step, guided by the model and your prompts.\n- It effectively controls the evolution of the image generation process, ensuring the final result aligns with your input prompts.\n\n2.- Customization: You can adjust various parameters to fine-tune the output, such as:\n- Sampling Steps: The number of denoising iterations. More steps typically yield better-quality images (up to a point) but take longer to process.\n- CFG Scale (Classifier-Free Guidance Scale): Controls how strongly the model adheres to the prompt. Higher values enforce more alignment but can also reduce creativity or realism.\n- Seed: Determines the randomness of the generated image. The same seed will consistently reproduce the same image given the same prompt and settings.\n\n3.- Algorithms:\n- The \"K\" in KSampler often refers to Karras-style samplers, which are specific sampling methods used in Stable Diffusion. These algorithms define how noise is reduced and include options like: euler, euler a, heun, etc.\n- Each sampling algorithm has its own characteristics, influencing the style, speed, and quality of the generated images.\n\nHow It Fits in the Workflow:\n1.- Initial Noise: The process starts with a noisy latent image.\n2.- Denoising Steps: The KSampler iteratively reduces the noise, using the selected algorithm and guided by the prompt and model weights.\n3.- Output Latent Image: After the specified steps, the KSampler produces a refined latent representation.\n4.- Decoding: The VAE decodes the latent representation into a final, viewable image."
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 45,
      "type": "Note",
      "pos": [
        1326,
        204
      ],
      "size": [
        497.57916259765625,
        293.83526611328125
      ],
      "flags": {},
      "order": 9,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "Note - Vae Decode",
      "properties": {},
      "widgets_values": [
        "In tools like ComfyUI or other systems using Stable Diffusion, the vae_decode function or node refers to the process of converting the latent representation of an image (also called a latent image) back into a viewable, full-resolution image.\n\nThis step is crucial because the model works in latent space, which is a compressed and abstract representation of the image, but to actually see and use the image, it needs to be decoded into pixel space.\n\nHow vae_decode Works:\n1.- Input:\n- The process starts with a latent image, which is the compressed output from the diffusion model.\n- This latent image is not directly viewable as it exists in a lower-dimensional mathematical form.\n\n2.- Decoding:\n- The VAE (Variational Autoencoder) takes this latent image and reconstructs it into a pixel-space image.\n- During this step, the VAE decodes the compressed features into detailed textures, colors, and structures to produce the final image.\n\n3.-Output:\n- The result is a high-resolution, viewable image with all the details fully restored.\n\nWhy Is vae_decode Important?\n- Bridges the Gap: The Stable Diffusion model operates in latent space for efficiency, but decoding is necessary to visualize the result in pixel space.\n- Enhances Quality: A good VAE ensures accurate decoding, which improves color fidelity, sharpness, and reduces artifacts in the final image.\n- Flexibility: Some workflows allow custom VAEs for different styles or effects, offering more control over how the latent data is decoded.\n\nWorkflow Context:\n- The model generates a latent image by transforming noise guided by your prompt.\n- The latent image is passed through the vae_decode step.\n- The vae_decode produces the final, usable image that can be saved or edited."
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 2,
      "type": "CheckpointLoaderSimple",
      "pos": [
        34.639862060546875,
        93.48159790039062
      ],
      "size": [
        326.40057373046875,
        101.81242370605469
      ],
      "flags": {},
      "order": 10,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [
            54
          ],
          "slot_index": 0,
          "shape": 3
        },
        {
          "name": "CLIP",
          "type": "CLIP",
          "links": [
            72,
            73
          ],
          "slot_index": 1,
          "shape": 3
        },
        {
          "name": "VAE",
          "type": "VAE",
          "links": null,
          "shape": 3
        }
      ],
      "properties": {
        "Node name for S&R": "CheckpointLoaderSimple"
      },
      "widgets_values": [
        "sdxl\\Pony\\Anime\\0\\30104_v70.safetensors"
      ]
    }
  ],
  "links": [
    [
      54,
      2,
      0,
      25,
      0,
      "MODEL"
    ],
    [
      57,
      8,
      0,
      26,
      1,
      "VAE"
    ],
    [
      65,
      35,
      0,
      25,
      3,
      "LATENT"
    ],
    [
      67,
      37,
      0,
      25,
      2,
      "CONDITIONING"
    ],
    [
      68,
      36,
      0,
      25,
      1,
      "CONDITIONING"
    ],
    [
      70,
      25,
      0,
      26,
      0,
      "LATENT"
    ],
    [
      71,
      26,
      0,
      27,
      0,
      "IMAGE"
    ],
    [
      72,
      2,
      1,
      36,
      0,
      "CLIP"
    ],
    [
      73,
      2,
      1,
      37,
      0,
      "CLIP"
    ]
  ],
  "groups": [
    {
      "id": 4,
      "title": "Ksampler",
      "bounding": [
        804.328857421875,
        554.5413208007812,
        682.7503662109375,
        716.5479125976562
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 5,
      "title": "Prompts",
      "bounding": [
        12.396934509277344,
        549.082275390625,
        761.1819458007812,
        658.593994140625
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 6,
      "title": "Checkpoint",
      "bounding": [
        12.037181854248047,
        18.64227294921875,
        364.58074951171875,
        499.1059875488281
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 7,
      "title": "Vae",
      "bounding": [
        808.8140258789062,
        28.02798843383789,
        470.4630126953125,
        491.4908142089844
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 9,
      "title": "Latent Image",
      "bounding": [
        416.0453796386719,
        20.962745666503906,
        357.50390625,
        509.3843078613281
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 10,
      "title": "Vae Decode",
      "bounding": [
        1301.8111572265625,
        31.756263732910156,
        538.9032592773438,
        484.0710144042969
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    }
  ],
  "config": {},
  "extra": {
    "ds": {
      "scale": 0.9646149645000018,
      "offset": [
        381.58307406373433,
        -30.36884759007104
      ]
    },
    "ue_links": []
  },
  "version": 0.4
}